{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b088c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "try:\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    statsmodels_available = True\n",
    "except ImportError:\n",
    "    statsmodels_available = False\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Load the dataset\n",
    "def load_data(url):\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        print(\"Data loaded successfully.\")\n",
    "        print(\"\\nDataset Column Types:\")\n",
    "        print(df.dtypes)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\"\n",
    "df = load_data(url)\n",
    "if df is None:\n",
    "    raise SystemExit(\"Exiting due to data loading failure.\")\n",
    "\n",
    "# Subsample dataset for faster training (comment out to use full dataset)\n",
    "df = df.sample(frac=0.3, random_state=42)\n",
    "print(f\"\\nSubsampled dataset to {len(df)} rows for faster training.\")\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    print(\"\\n=== Data Preprocessing ===\")\n",
    "\n",
    "    # Display missing values before imputation\n",
    "    print(\"Missing Values Before Imputation:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Separate categorical and numerical columns\n",
    "    categorical_cols = ['ocean_proximity'] if 'ocean_proximity' in df.columns else []\n",
    "    numerical_cols = [col for col in df.columns if col not in categorical_cols]\n",
    "\n",
    "    # Debug: Confirm columns\n",
    "    print(\"\\nNumerical Columns for Imputation:\", numerical_cols)\n",
    "    print(\"Categorical Columns:\", categorical_cols)\n",
    "\n",
    "    # Handle missing values with KNN imputation for numerical columns\n",
    "    try:\n",
    "        if numerical_cols:\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            df[numerical_cols] = pd.DataFrame(\n",
    "                imputer.fit_transform(df[numerical_cols]),\n",
    "                columns=numerical_cols,\n",
    "                index=df.index\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during KNN imputation: {e}\")\n",
    "        print(\"Falling back to median imputation.\")\n",
    "        for col in numerical_cols:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    # Display missing values after imputation\n",
    "    print(\"\\nMissing Values After Imputation:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Remove duplicates\n",
    "    initial_rows = len(df)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"\\nDuplicates Removed: {initial_rows - len(df)}\")\n",
    "\n",
    "    # Cap outliers for numerical columns and track counts\n",
    "    outlier_counts = {}\n",
    "    def cap_outliers(series, col_name):\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = ((series < lower_bound) | (series > upper_bound)).sum()\n",
    "        outlier_counts[col_name] = outliers\n",
    "        return series.clip(lower_bound, upper_bound)\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        df[col] = cap_outliers(df[col], col)\n",
    "\n",
    "    print(\"\\nOutliers Detected and Capped:\")\n",
    "    for col, count in outlier_counts.items():\n",
    "        print(f\"{col}: {count} outliers\")\n",
    "\n",
    "    # Display skewness before log-transformation\n",
    "    print(\"\\nSkewness Before Log-Transformation:\")\n",
    "    print(df[numerical_cols].skew())\n",
    "\n",
    "    # Log-transform skewed features\n",
    "    skewed_cols = ['total_rooms', 'population', 'median_house_value']\n",
    "    for col in skewed_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.log1p(df[col])\n",
    "\n",
    "    # Display skewness after log-transformation\n",
    "    print(\"\\nSkewness After Log-Transformation:\")\n",
    "    print(df[numerical_cols].skew())\n",
    "\n",
    "    # Verify median_house_value presence\n",
    "    print(\"\\nColumns After Preprocessing:\", df.columns.tolist())\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# 3. Feature Engineering\n",
    "def engineer_features(df):\n",
    "    print(\"\\n=== Feature Engineering ===\")\n",
    "    # Add core features (reduced set)\n",
    "    df['rooms_per_household'] = df['total_rooms'] / df['households']\n",
    "    df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']\n",
    "    df['population_per_household'] = df['population'] / df['households']\n",
    "    # Removed 'distance_to_coast' and 'median_income_poly1' to reduce feature count\n",
    "\n",
    "    # Summarize new features\n",
    "    print(\"New Features Created:\")\n",
    "    print(df[['rooms_per_household', 'bedrooms_per_room', 'population_per_household']].head())\n",
    "\n",
    "    return df\n",
    "\n",
    "df = engineer_features(df)\n",
    "\n",
    "# 4. Statistical Analysis\n",
    "def statistical_analysis(df):\n",
    "    print(\"\\n=== Statistical Analysis ===\")\n",
    "    stat, p_value = stats.shapiro(df['median_house_value'])\n",
    "    normality_result = f\"Shapiro-Wilk Test for median_house_value: p-value = {p_value:.4f}\"\n",
    "\n",
    "    # Filter numerical columns for descriptive stats, skewness, and kurtosis\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    print(\"\\nNumerical Columns for Statistical Analysis:\", numerical_cols.tolist())\n",
    "\n",
    "    desc_stats = df[numerical_cols].describe().T\n",
    "    desc_stats['skewness'] = df[numerical_cols].skew()\n",
    "    desc_stats['kurtosis'] = df[numerical_cols].kurtosis()\n",
    "\n",
    "    # Multcollinearity analysis (VIF)\n",
    "    if statsmodels_available:\n",
    "        numerical_df = df[numerical_cols]\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data['Feature'] = numerical_df.columns\n",
    "        vif_data['VIF'] = [variance_inflation_factor(numerical_df.values, i) for i in range(numerical_df.shape[1])]\n",
    "        print(\"\\nVariance Inflation Factor (VIF) Analysis:\")\n",
    "        print(vif_data)\n",
    "\n",
    "    return normality_result, desc_stats\n",
    "\n",
    "normality_result, desc_stats = statistical_analysis(df)\n",
    "print(\"\\nNormality Test:\")\n",
    "print(normality_result)\n",
    "print(\"\\nDescriptive Statistics with Skewness and Kurtosis:\")\n",
    "print(desc_stats)\n",
    "\n",
    "# 5. Exploratory Data Analysis (EDA)\n",
    "def perform_eda(df):\n",
    "    print(\"\\n=== Exploratory Data Analysis ===\")\n",
    "    # Correlation matrix\n",
    "    corr_matrix = df.select_dtypes(include=['float64', 'int64']).corr()\n",
    "    fig_corr = go.Figure(data=go.Heatmap(\n",
    "        z=corr_matrix.values,\n",
    "        x=corr_matrix.columns,\n",
    "        y=corr_matrix.columns,\n",
    "        colorscale='RdBu',\n",
    "        zmin=-1, zmax=1,\n",
    "        text=corr_matrix.values.round(2),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 10}\n",
    "    ))\n",
    "    fig_corr.update_layout(title='Interactive Correlation Matrix', width=800, height=800)\n",
    "    fig_corr.show()\n",
    "\n",
    "    # Scatter plot (fixed syntax error)\n",
    "    fig_scatter = px.scatter(df, x='median_income', y='median_house_value', title='Median Income vs Median House Value (USD)',\n",
    "                            hover_data=['longitude', 'latitude'],\n",
    "                            trendline='ols')\n",
    "    fig_scatter.update_yaxes(title_text='Median House Value (USD)')\n",
    "    fig_scatter.update_traces(hovertemplate='Income: %{x}<br>House Value: $%{y:,.2f} USD')\n",
    "    fig_scatter.show()\n",
    "\n",
    "    # Geographical plot (requires Mapbox token)\n",
    "    px.set_mapbox_access_token('your_mapbox_token')\n",
    "    fig_geo = px.scatter_mapbox(df, lat='latitude', lon='longitude', color='median_house_value',\n",
    "                                size='population', zoom=5, mapbox_style='open-street-map',\n",
    "                                title='House Prices by Location (USD)',\n",
    "                                color_continuous_scale=px.colors.sequential.Plasma)\n",
    "    fig_geo.update_coloraxes(colorbar_title='Median House Value (USD)')\n",
    "    fig_geo.show()\n",
    "\n",
    "    # Distribution plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['median_house_value'], kde=True)\n",
    "    plt.title('Distribution of Median House Value (USD)')\n",
    "    plt.xlabel('Median House Value (USD)')\n",
    "    plt.show()\n",
    "\n",
    "perform_eda(df)\n",
    "\n",
    "# 6. Prepare Data for Modeling\n",
    "def prepare_data(df):\n",
    "    X = df.drop('median_house_value', axis=1)\n",
    "    y = df['median_house_value']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_data(df)\n",
    "\n",
    "# 7. Modeling and Impact Analysis\n",
    "def build_pipeline(model):\n",
    "    numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_cols = ['ocean_proximity'] if 'ocean_proximity' in df.columns else []\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def plot_feature_importance(model, feature_names):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curves(model, X_train, y_train):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X_train, y_train, cv=3, scoring='r2', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5))\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='Training R²')\n",
    "    plt.plot(train_sizes, val_mean, label='Validation R²')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
    "    plt.xlabel('Training Examples')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    print(\"\\n=== Model Training and Evaluation ===\")\n",
    "\n",
    "    # Define models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=50, random_state=42)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_r2 = -np.inf\n",
    "\n",
    "    numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_cols = ['ocean_proximity'] if 'ocean_proximity' in df.columns else []\n",
    "    feature_names = numerical_cols.tolist() + [f\"ocean_proximity_{cat}\" for cat in OneHotEncoder(drop='first').fit(X_train[categorical_cols]).get_feature_names_out()] if categorical_cols else numerical_cols.tolist()\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        pipeline = build_pipeline(model)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        # Evaluation metrics (convert back to USD)\n",
    "        y_test_usd = np.expm1(y_test)\n",
    "        y_pred_usd = np.expm1(y_pred)\n",
    "        mae = mean_absolute_error(y_test_usd, y_pred_usd)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_usd, y_pred_usd))\n",
    "        r2 = r2_score(y_test_usd, y_pred_usd)\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'MAE (USD)': mae,\n",
    "            'RMSE (USD)': rmse,\n",
    "            'R²': r2\n",
    "        })\n",
    "\n",
    "        print(f\"\\n{name} Performance (in USD):\")\n",
    "        print(f\"Mean Absolute Error: ${mae:,.2f}\")\n",
    "        print(f\"Root Mean Squared Error: ${rmse:,.2f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "        # Feature importance (for Random Forest and Gradient Boosting)\n",
    "        if name in ['Random Forest', 'Gradient Boosting']:\n",
    "            plot_feature_importance(pipeline.named_steps['regressor'], feature_names)\n",
    "\n",
    "        # Learning curves\n",
    "        plot_learning_curves(pipeline, X_train, y_train)\n",
    "\n",
    "        # Residual analysis\n",
    "        residuals = y_test_usd - y_pred_usd\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "        plt.title(f'Q-Q Plot of Residuals ({name})')\n",
    "        plt.show()\n",
    "\n",
    "        # Update best model\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model = pipeline\n",
    "\n",
    "    # Display model comparison\n",
    "    print(\"\\n=== Model Comparison ===\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "best_model = train_and_evaluate(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 8. Gradio Interface for Hugging Face Spaces\n",
    "try:\n",
    "    import gradio as gr\n",
    "\n",
    "    def predict_house_value(longitude, latitude, housing_median_age, total_rooms, total_bedrooms,\n",
    "                            population, households, median_income, ocean_proximity):\n",
    "        input_data = pd.DataFrame({\n",
    "            'longitude': [longitude],\n",
    "            'latitude': [latitude],\n",
    "            'housing_median_age': [housing_median_age],\n",
    "            'total_rooms': [np.log1p(total_rooms)],\n",
    "            'total_bedrooms': [total_bedrooms],\n",
    "            'population': [np.log1p(population)],\n",
    "            'households': [households],\n",
    "            'median_income': [median_income],\n",
    "            'ocean_proximity': [ocean_proximity],\n",
    "            'rooms_per_household': [np.log1p(total_rooms) / households],\n",
    "            'bedrooms_per_room': [total_bedrooms / np.log1p(total_rooms)],\n",
    "            'population_per_household': [np.log1p(population) / households]\n",
    "        })\n",
    "        prediction = best_model.predict(input_data)\n",
    "        usd_value = np.expm1(prediction[0])  # Reverse log transformation\n",
    "        return f\"Predicted House Value: ${usd_value:,.2f} USD\"\n",
    "\n",
    "    iface = gr.Interface(\n",
    "        fn=predict_house_value,\n",
    "        inputs=[\n",
    "            gr.Slider(-124, -114, step=0.1, label=\"Longitude\"),\n",
    "            gr.Slider(32, 42, step=0.1, label=\"Latitude\"),\n",
    "            gr.Slider(0, 52, step=1, label=\"Housing Median Age\"),\n",
    "            gr.Slider(0, 40000, step=100, label=\"Total Rooms\"),\n",
    "            gr.Slider(0, 7000, step=10, label=\"Total Bedrooms\"),\n",
    "            gr.Slider(0, 50000, step=100, label=\"Population\"),\n",
    "            gr.Slider(0, 7000, step=10, label=\"Households\"),\n",
    "            gr.Slider(0, 15, step=0.1, label=\"Median Income\"),\n",
    "            gr.Dropdown(choices=['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND'], label=\"Ocean Proximity\")\n",
    "        ],\n",
    "        outputs=\"text\",\n",
    "        title=\"California House Price Predictor\",\n",
    "        description=\"Enter features to predict the median house value (in USD).\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nGradio interface is ready. Run iface.launch() in a Hugging Face Space to use it.\")\n",
    "except ImportError:\n",
    "    print(\"\\nGradio not installed. Skipping Gradio interface. Install gradio to enable.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
